{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for chiller cooling from BDX data, where the setpoints are generated from a simple feedback controller (`src/baseline_control.SimpleFeedbackController`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from itertools import zip_longest, product\n",
    "\n",
    "import notebook_setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tslearn.utils import to_time_series\n",
    "from tslearn.metrics import dtw\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "from sklearn import manifold\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import gym\n",
    "import optuna\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "from bdx import get_trend\n",
    "from utils import get_credentials\n",
    "from plotting import model_surface, plot_surface\n",
    "from commonml import rl, helpers, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data\n",
    "\n",
    "* Chiller 1 data (2422) `2021-03-18 1220-5` to `2021-03-26 1010-5`, `2021-05-25 0000-5` to `2021-06-08 0000-5`\n",
    "* Chiller 2 data (2841) `2021-03-26 1010-5` to `2021-05-16 0610-5`\n",
    "* Python setpoint data (3481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cst = timezone(offset=-timedelta(hours=6))\n",
    "cdt = timezone(offset=-timedelta(hours=5))\n",
    "\n",
    "datadir = os.path.join(os.environ.get('DATADIR'), 'EngineeringScienceBuilding')\n",
    "username, password = get_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Download and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get data from BDX\n",
    "ch1 = get_trend('2422', username, password,\n",
    "               start=datetime(2021,3,18,12,20, tzinfo=cdt),\n",
    "               end=datetime(2021,3,26,10,10, tzinfo=cdt))\n",
    "ch1 = ch1.append(get_trend('2422', username, password,\n",
    "                 start=datetime(2021,5,25,0,0, tzinfo=cdt),\n",
    "                 end=datetime(2021,6,8,0,0, tzinfo=cdt)))\n",
    "\n",
    "ch2 = get_trend('2841', username, password,\n",
    "               start=datetime(2021,3,26,10,10, tzinfo=cdt),\n",
    "               end=datetime(2021,5,16,6,10, tzinfo=cdt))\n",
    "\n",
    "stpt = get_trend('3481', username, password,\n",
    "               start=datetime(2021,3,18,12,20, tzinfo=cdt),\n",
    "               end=datetime(2021,5,16,6,10, tzinfo=cdt))\n",
    "stpt = stpt.append(get_trend('3481', username, password,\n",
    "                   start=datetime(2021,5,25,0,0, tzinfo=cdt),\n",
    "                   end=datetime(2021,6,8,0,0, tzinfo=cdt)))\n",
    "stpt = stpt['CDWTPythonSetpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get naive reference data from BDX\n",
    "refstart, refend = datetime(2020,3,1, tzinfo=cst), datetime(2021,3,8, tzinfo=cst)\n",
    "stpt_ref = get_trend('3481', username, password,\n",
    "               start=refstart, end=refend)\n",
    "\n",
    "stpt_ref = stpt_ref['JCI Cooling_Tower_Water_Setpoint']\n",
    "\n",
    "ch1_ref = get_trend('2422', username, password,\n",
    "                   start=refstart, end=refend)\n",
    "ch1_ref = ch1_ref.loc[(ch1_ref['RunChi']==True) & (ch1_ref['PowChi'] > 0)]\n",
    "\n",
    "ch2_ref = get_trend('2841', username, password,\n",
    "                   start=refstart, end=refend)\n",
    "ch2_ref = ch2_ref.loc[(ch2_ref['RunChi']==True) & (ch2_ref['PowChi'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save data to disk\n",
    "ch1.to_csv(os.path.join(datadir, '2422_v3_chiller1_model.csv'))\n",
    "ch2.to_csv(os.path.join(datadir, '2841_v3_chiller2_model.csv'))\n",
    "stpt.to_csv(os.path.join(datadir, '3481_v3_setpoint_model.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save reference data to disk\n",
    "ch1_ref.to_csv(os.path.join(datadir, '2422_v3_chiller1_eval.csv'))\n",
    "ch2_ref.to_csv(os.path.join(datadir, '2841_v3_chiller2_eval.csv'))\n",
    "stpt_ref.to_csv(os.path.join(datadir, '3481_v3_setpoint_eval.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load data from disk\n",
    "ch1 = pd.read_csv(os.path.join(datadir, '2422_v3_chiller1_model.csv'), index_col='time', parse_dates=True)\n",
    "ch2 = pd.read_csv(os.path.join(datadir, '2841_v3_chiller2_model.csv'), index_col='time', parse_dates=True)\n",
    "stpt = pd.read_csv(os.path.join(datadir, '3481_v3_setpoint_model.csv'), index_col='time', parse_dates=True)\n",
    "\n",
    "ch1_stpt = stpt.loc[ch1.index]\n",
    "ch2_stpt = stpt.loc[ch2.index]\n",
    "ch1['Setpoint'] = ch1_stpt\n",
    "ch2['Setpoint'] = ch2_stpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load ref data from disk\n",
    "ch1_ref = pd.read_csv(os.path.join(datadir, '2422_v3_chiller1_eval.csv'), index_col='time', parse_dates=True)\n",
    "ch2_ref = pd.read_csv(os.path.join(datadir, '2841_v3_chiller2_eval.csv'), index_col='time', parse_dates=True)\n",
    "stpt_ref = pd.read_csv(os.path.join(datadir, '3481_v3_setpoint_eval.csv'), index_col='time', parse_dates=True)\n",
    "\n",
    "ch1_ref_stpt = stpt_ref.loc[ch1_ref.index]\n",
    "ch2_ref_stpt = stpt_ref.loc[ch2_ref.index]\n",
    "ch1_ref['Setpoint'] = ch1_ref_stpt\n",
    "ch2_ref['Setpoint'] = ch2_ref_stpt\n",
    "\n",
    "ch1_ref = ch1_ref[~ch1_ref['Setpoint'].isna()]\n",
    "ch2_ref = ch2_ref[~ch2_ref['Setpoint'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Models & Environments\n",
    "\n",
    "Time-independent state space. `[x]-> y` where `[Ambient, Chiller, Setpoint] -> [Next Condenser Water Temp]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from systems import CoolingTowerEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ch1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Features for the data-model for environment\n",
    "# State + action variables\n",
    "colsx = [\n",
    "    # Ambient\n",
    "    'TempWetBulb', 'TempAmbient',\n",
    "    # Machine temperatures\n",
    "    'TempCondOut',\n",
    "    # Machine state\n",
    "    'Tonnage', 'PressDiffCond',\n",
    "    # Action\n",
    "    'Setpoint'\n",
    "]\n",
    "state_vars = colsx[:-1]\n",
    "# Variables for cooling tower conditions that are staged\n",
    "ticker_vars = ['TempWetBulb', 'TempAmbient', 'Tonnage', 'PressDiffCond']\n",
    "lag = 1\n",
    "colsy = ['TempCondIn', 'TempCondOut', 'PowFanA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_env_data(df, colsx, colsy, ticker_vars, lag=1, train_split=0.9):\n",
    "    data = df.loc[:, list(set(colsx+colsy+ticker_vars))]\n",
    "    data = data.dropna(axis=0, how='any')\n",
    "    ticker = [day_data for date, day_data in data[ticker_vars].groupby(data.index.date)]\n",
    "    ticker = [t for t in ticker if len(t)==288] # all samples for a day\n",
    "    x = data.loc[data.index[:-lag], colsx]\n",
    "    y = data.loc[data.index[lag:], colsy]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    x = scaler.fit_transform(x)\n",
    "    y = y.to_numpy().squeeze()\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=train_split)\n",
    "    return x_train, x_val, y_train, y_val, ticker, scaler\n",
    "\n",
    "def train_model(x, y, **model_params):\n",
    "    params = dict(hidden_layer_sizes=(32,32,32), learning_rate_init=1e-3, max_iter=500, verbose=True)\n",
    "    params.update(model_params)\n",
    "    model = MLPRegressor(**params)\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "def get_env(model_fn, scaler_fn, ticker, seed=0):\n",
    "    if isinstance(model_fn, sklearn.base.BaseEstimator):\n",
    "        model_fn = model_fn.predict\n",
    "    if isinstance(scaler_fn, sklearn.base.TransformerMixin):\n",
    "        scaler_fn = scaler_fn.transform\n",
    "    return CoolingTowerEnv(model_fn, ticker, seed, scaler_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ESB Chiller 1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train1, x_val1, y_train1, y_val1, ticker1, scaler1 = get_env_data(ch1, colsx, colsy, ticker_vars)\n",
    "model1 = train_model(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xx = x_val1[:200]\n",
    "xx_ = scaler1.inverse_transform(xx)\n",
    "yy = y_val1[:200]\n",
    "yp = model1.predict(xx)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(yp[:,0], label='TempCondIn-pred')\n",
    "plt.plot(yp[:,1], label='TempCondOut-pred')\n",
    "plt.plot(xx_[:,0], label='TempWetBulb', ls=':')\n",
    "plt.plot(xx_[:,2], label='TempCondOut-last', ls=':')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(yy[:,2], label='PowFan-pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "stpts = [55., 60., 65., 70., 75.]\n",
    "for stpt in stpts:\n",
    "    env1 = get_env(model1, scaler1, ticker1, 0)\n",
    "    scaled_stpt = env1.scale_setpoint([stpt])\n",
    "    rewards = helpers.rewards(env1, lambda x: scaled_stpt)[0]\n",
    "    plt.plot(rewards, label='{:.0f}, total:{:.0f}'.format(stpt, sum(rewards)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "TODO:\n",
    "\n",
    "1. Mutual information between TempCondIn-Pred vs TempCondIn, and TempCondIn vs TempCondIn-Last. Essentially figure out if the model prediction is closer to the actual data, or whether the data between lags is closer to each other)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ESB Chiller 2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train2, x_val2, y_train2, y_val2, ticker2, scaler2 = get_env_data(ch2, colsx, colsy, ticker_vars)\n",
    "model2 = train_model(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx = data.loc[data.index[:2000], colsx]\n",
    "xx = scaler2.transform(xx)\n",
    "yy = model2.predict(xx)\n",
    "plt.figure(figsize=(10,5))\n",
    "# plt.plot(np.arange(len(yy)), yy[:, 0], label='TempCondIn-Pred')\n",
    "# plt.plot(np.arange(len(yy)), ch2.loc[ch2.index[1:501], 'TempCondIn'].to_numpy().squeeze(), label='TempCondIn', ls=':')\n",
    "# plt.plot(np.arange(len(yy)), ch2.loc[ch2.index[:500], 'TempCondIn'].to_numpy().squeeze(), label='TempCondIn-Last', ls=':')\n",
    "# plt.legend()\n",
    "# plt.twinx()\n",
    "plt.plot(np.arange(len(yy)), yy[:, 2], label='PowFanA-Pred')\n",
    "plt.plot(np.arange(len(yy)), ch2.loc[data.index[1:2001], 'PowFanA'].to_numpy().squeeze(), label='PowFanA', ls=':')\n",
    "plt.plot(np.arange(len(yy)), ch2.loc[data.index[0:2000], 'PowFanA'].to_numpy().squeeze(), label='PowFanA-Last', ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "stpts = [55., 60., 65., 70., 75.]\n",
    "for stpt in stpts:\n",
    "    env2 = get_env(model2, scaler2, ticker2, 0)\n",
    "    scaled_stpt = env2.scale_setpoint([stpt])\n",
    "    rewards = helpers.rewards(env2, lambda x: scaled_stpt)[0]\n",
    "    plt.plot(rewards, label='{:.0f}, total:{:.0f}'.format(stpt, sum(rewards)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Benchmark Controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from controllers.baseline_control import SimpleFeedbackController\n",
    "\n",
    "class UpDownController(SimpleFeedbackController):\n",
    "    \n",
    "    def __init__(self, model_fn, scaler_fn, seed=None):\n",
    "        super().__init__(bounds=((-1, 1.),), stepsize=0.05, window=1, seed=seed)\n",
    "        self.model_fn = model_fn\n",
    "        self.scaler_fn = scaler_fn\n",
    "    \n",
    "    def feedback(self, newstate):\n",
    "        if len(self._actions) == 0:\n",
    "            return 0.\n",
    "        state = self._states[-1]\n",
    "        x = np.concatenate((state, self._actions[-1]))\n",
    "        x = self.scaler_fn(x.reshape(1, -1))\n",
    "        x[-1] = self._actions[-1][0] # action is in [-1,1] range already\n",
    "        temp_cond_in, temp_cond_out, pow_fan = self.model_fn(x)[0]\n",
    "        reward = ((state[2] - temp_cond_in) / \\\n",
    "                  (state[2] - state[0] + 1e-2)) - \\\n",
    "                 (max(pow_fan, 0) / 10)\n",
    "        return np.clip(reward, -1, 1)\n",
    "\n",
    "    def starting_action(self, x):\n",
    "        return self.random.uniform(*self.bounds[0])\n",
    "\n",
    "class FixedApproachController:\n",
    "    \n",
    "    def __init__(self, approach: float):\n",
    "        self.approach = approach\n",
    "    \n",
    "    def predict(self, state):\n",
    "        wetbulb = state[0]\n",
    "        setpoint = np.array((self.approach + wetbulb,))\n",
    "        return (setpoint - 65) / 10., 1.\n",
    "\n",
    "class ModelPredictiveController:\n",
    "    \n",
    "    def __init__(self, model_fn, scaler_fn, resolution=21):\n",
    "        self.model_fn = model_fn\n",
    "        self.scaler_fn = scaler_fn\n",
    "        self.resolution = resolution\n",
    "        self.bounds = np.asarray([[-1, 1]])\n",
    "        self.actions = np.linspace(self.bounds[0][0], self.bounds[0][1], num=resolution, endpoint=True).reshape(-1, 1)\n",
    "\n",
    "    def predict(self, state):\n",
    "        x = np.tile(state, (self.resolution, 1))\n",
    "        x = np.hstack((x, self.actions))\n",
    "        x = self.scaler_fn(x)\n",
    "        x[:, -1] = self.actions[:, 0]\n",
    "        y = self.model_fn(x)\n",
    "        temp_cond_in, temp_cond_out, pow_fan = y[:,0], y[:,1], y[:,2]\n",
    "        reward = ((state[2] - temp_cond_in) / \\\n",
    "                  (state[2] - state[0] + 1e-2)) - \\\n",
    "                 (np.fmax(pow_fan, 0) / 10)\n",
    "        best = np.argmax(reward)\n",
    "        return self.actions[best], 1.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RL hyperparameter search\n",
    "def objective(trial: optuna.Trial):\n",
    "    params = dict(\n",
    "        policy = rl.ActorCriticBox,\n",
    "        activation = nn.Tanh,\n",
    "        state_dim = len(state_vars),\n",
    "        action_dim = 1,\n",
    "        n_latent_var = trial.suggest_int('n_latent_var', 16, 128),\n",
    "        lr = trial.suggest_loguniform('lr', 1e-4, 1e-1),\n",
    "        gamma = 0.,\n",
    "        epochs = trial.suggest_int('epochs', 1, 10),\n",
    "        update_interval = trial.suggest_int('update_interval', 16, 288, log=True)\n",
    "    )\n",
    "    \n",
    "    env = get_env(model2, scaler2, ticker2, 0)\n",
    "    agent = rl.PPO(env = env, seed=0, **params)\n",
    "    rewards = agent.learn(timesteps=trial.suggest_int('timesteps', 288*10, 288*30),\n",
    "                          reward_aggregation='episodic.normalized')\n",
    "    feedback = np.mean(rewards[-5:])\n",
    "    return feedback\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent_params = dict(\n",
    "    policy = rl.ActorCriticBox,\n",
    "    activation = nn.Tanh,\n",
    "    state_dim = len(state_vars),\n",
    "    action_dim = 1,\n",
    "    n_latent_var = 64,\n",
    "    lr = 3e-4,\n",
    "    gamma = 0.,\n",
    "    epochs = 10,\n",
    "    update_interval = 150,\n",
    "    truncate=False,\n",
    "    seed=0\n",
    ")\n",
    "timesteps = 288 * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transfer_experiment(agent_from, agent_to, transfer_model=None, timesteps=288*30, **agent_params):\n",
    "    env_from, env_to = agent_from.env, agent_to.env\n",
    "    if transfer_model is not None:\n",
    "        env_model = get_env(transfer_model, env_to.scaler_fn, env_to.ticker_vars, 0)\n",
    "        agentx = rl.PPO(env=env_model, **agent_params)\n",
    "        agentx.policy.load_state_dict(agent_from.policy.state_dict())\n",
    "        agentx.learn(timesteps // 10)\n",
    "        agentx.env = get_env(transfer_model, env_to.scaler_fn, env_to.ticker_vars, 0)\n",
    "        rewardsx = agentx.learn(timesteps=timesteps)\n",
    "    else:\n",
    "        agentx, rewardsx = None, None\n",
    "    \n",
    "    agentx_ = rl.PPO(env=None, **agent_params)\n",
    "    agentx_.env = get_env(env_to.model_fn, env_to.scaler_fn, env_to.ticker_vars, 0)\n",
    "    agentx_.policy.load_state_dict(agent_from.policy.state_dict())\n",
    "    rewardsx_ = agentx_.learn(timesteps=timesteps)\n",
    "    \n",
    "    return (agentx, rewardsx), (agentx_, rewardsx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Transfer Across Towers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Env 1 Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Training on environment 1\n",
    "env1 = get_env(model1, scaler1, ticker1, 0)\n",
    "agent1 = rl.PPO(env = env1, **agent_params)\n",
    "rewards1 = agent1.learn(timesteps=timesteps)\n",
    "plt.plot(stats.rolling_mean(rewards1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "env1_ = get_env(model1, scaler1, ticker2[:1], seed=0)\n",
    "res = helpers.get_from_env(('reward', 'action'), env1_, lambda s: agent1.policy.predict(s)[0])\n",
    "actions = np.asarray(res['action']).squeeze()\n",
    "rewards = np.asarray(res['reward']).squeeze()\n",
    "actions = np.asarray(actions) * 10 + 65\n",
    "# plt.figure(figsize=(10,5))\n",
    "s, = plt.plot(actions)\n",
    "plt.ylabel('setpoint')\n",
    "plt.ylim(55, 75)\n",
    "plt.twinx()\n",
    "r, = plt.plot(rewards, c='r', ls=':')\n",
    "plt.ylabel('reward')\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend((s, r), ('Setpoint', 'Reward'))\n",
    "plt.title('Tower 1 control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Env 2 Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Training environment 2\n",
    "env2 = get_env(model2, scaler2, ticker2, 0)\n",
    "agent2 = rl.PPO(env = env2, **agent_params)\n",
    "rewards2 = agent2.learn(timesteps=timesteps)\n",
    "plt.plot(stats.rolling_mean(rewards2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "env2_ = get_env(model2, scaler2, ticker2[:1], seed=0)\n",
    "res = helpers.get_from_env(('reward', 'action'), env2_, lambda s: agent2.policy.predict(s)[0])\n",
    "actions = np.asarray(res['action']).squeeze()\n",
    "rewards = np.asarray(res['reward']).squeeze()\n",
    "actions = np.asarray(actions) * 10 + 65\n",
    "# plt.figure(figsize=(10,5))\n",
    "s, = plt.plot(actions)\n",
    "plt.ylabel('setpoint')\n",
    "plt.ylim(55, 75)\n",
    "plt.twinx()\n",
    "r, = plt.plot(rewards, c='r', ls=':')\n",
    "plt.ylabel('reward')\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend((s, r), ('Setpoint', 'Reward'))\n",
    "plt.title('Tower 2 control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Model Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_point = x_val2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model 1 prediction with model 2 data\n",
    "vary = (0, 5)\n",
    "s1 = model_surface(lambda x: model1.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "s2 = model_surface(lambda x: model2.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "\n",
    "# fitting model1 on model2 data\n",
    "model12 = helpers.clone(model1, dict(n_iter_=0, t_=0, loss_curve_=[], best_loss_=np.inf))\n",
    "model12.set_params(warm_start=True, verbose=True)\n",
    "model12.fit(x_val2, y_val2)\n",
    "# model12.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "s12 = model_surface(lambda x: model12.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "\n",
    "plot_surface(*s1, fig_kwargs={'figsize':(8,8)}, cmap=plt.cm.coolwarm, hatch='o', alpha=0.6)\n",
    "ax = plt.gca()\n",
    "plot_surface(*s2, cmap=plt.cm.coolwarm, ax=ax, hatch='-', alpha=0.6)\n",
    "plot_surface(*s12, cmap=plt.cm.coolwarm, ax=ax, hatch='.', alpha=0.6)\n",
    "ax.set_zlim(50, 80)\n",
    "plt.xlabel(colsx[vary[0]])\n",
    "plt.ylabel(colsx[vary[1]] if vary[1]<len(colsx) else 'Setpoint')\n",
    "ax.set_zlabel('Condenser Temperature In')\n",
    "ax.view_init(10, 60)\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Policy transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Transfer from env 1 to 2, using model12 as intermediate\n",
    "env12 = get_env(model12, scaler2, ticker2, 0)\n",
    "agent12 = rl.PPO(env = env12, **agent_params)\n",
    "agent12.policy.load_state_dict(agent1.policy.state_dict())\n",
    "agent12.learn(288 * 3)\n",
    "agent12.env = get_env(model12, scaler2, ticker2, 0)\n",
    "rewards12 = agent12.learn(timesteps=288*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Transfer w/o model as intermediate\n",
    "agent12_ = rl.PPO(env = env2, **agent_params)\n",
    "agent12_.policy.load_state_dict(agent1.policy.state_dict())\n",
    "rewards12_ = agent12_.learn(timesteps=288*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transferring across\n",
    "(agent12, rewards12), (agent12_, rewards12_) = \\\n",
    "    transfer_experiment(agent1, agent2, transfer_model=model12, timesteps=timesteps, **agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Learning performance\n",
    "plt.plot(stats.rolling_mean(rewards1, 4), label='Tower 1: %.2f' % np.mean(rewards1[-5:]), ls=':')\n",
    "plt.plot(stats.rolling_mean(rewards2, 4), label='Tower 2: %.2f' % np.mean(rewards2[-5:]))\n",
    "plt.plot(stats.rolling_mean(rewards12, 4), label='Tower 1->model->2: %.2f' % np.mean(rewards12[-5:]))\n",
    "plt.plot(stats.rolling_mean(rewards12_, 4), label='Tower 1->2: %.2f' % np.mean(rewards12_[-5:]))\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.title('RL Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Operational performance\n",
    "results = dict(r2=[], r12=[], r12_=[], rud=[], rfa=[], rmpc=[])\n",
    "for i in trange(10, leave=False):\n",
    "    _seed = i\n",
    "    # rprod1 = helpers.get_from_env(('reward',), get_env(model2, scaler2, ticker2, _seed), lambda x: agent1.predict(x)[0])['reward'][0]\n",
    "    results['r2'].append(helpers.get_from_env(('reward',), get_env(model2, scaler2, ticker2, _seed), lambda x: agent2.predict(x)[0])['reward'][0])\n",
    "    results['r12'].append(helpers.get_from_env(('reward',), get_env(model2, scaler2, ticker2, _seed), lambda x: agent12.predict(x)[0])['reward'][0])\n",
    "    results['r12_'].append(helpers.get_from_env(('reward',), get_env(model2, scaler2, ticker2, _seed), lambda x: agent12_.predict(x)[0])['reward'][0])\n",
    "\n",
    "    updown = UpDownController(model2.predict, scaler2.transform, 0)\n",
    "    results['rud'].append(helpers.get_from_env(('reward',), get_env(model2, scaler2, ticker2, _seed), lambda x: updown.predict(x)[0])['reward'][0])\n",
    "    fixed = FixedApproachController(approach=5)\n",
    "    results['rfa'].append(helpers.get_from_env(('reward',), get_env(model2, scaler2, ticker2, _seed), lambda x: fixed.predict(x)[0])['reward'][0])\n",
    "    mpc = ModelPredictiveController(modelb.predict, scalerb.transform, resolution=21)\n",
    "    results['rmpc'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: mpc.predict(x)[0])['reward'][0])\n",
    "\n",
    "keys = list(results.keys())\n",
    "resagg12 = {}\n",
    "for key in keys:\n",
    "    results[key] = helpers.homogenous_array(results[key])\n",
    "    resagg12[key+'_mean'] = np.nanmean(results[key], axis=0)\n",
    "    resagg12[key+'_std'] = np.nanstd(results[key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(stats.rolling_mean(rprod1, 4), label='Tower 1 {:.2f}'.format(sum(rprod1)), ls=':')\n",
    "r = resagg12\n",
    "p = plt.plot(stats.rolling_mean(r['r2_mean'], 4), label='Tower 2: {:.2f}'.format(sum(r['r2_mean'])), ls=':')\n",
    "plt.fill_between(np.arange(len(r['r2_mean'])), r['r2_mean']+r['r2_std'], r['r2_mean']-r['r2_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['r12_mean'], 4), label='Tower 1->model->2: {:.2f}'.format(sum(r['r12_mean'])))\n",
    "plt.fill_between(np.arange(len(r['r12_mean'])), r['r12_mean']+r['r12_std'], r['r12_mean']-r['r12_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['r12__mean'], 4), label='Tower 1->2: {:.2f}'.format(sum(r['r12__mean'])))\n",
    "plt.fill_between(np.arange(len(r['r12__mean'])), r['r12__mean']+r['r12__std'], r['r12__mean']-r['r12__std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rud_mean'], 4), label='UpDown: {:.2f}'.format(sum(r['rud_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rud_mean'])), r['rud_mean']+r['rud_std'], r['rud_mean']-r['rud_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rfa_mean'], 4), label='Fixed: {:.2f}'.format(sum(r['rfa_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rfa_mean'])), r['rfa_mean']+r['rfa_std'], r['rfa_mean']-r['rfa_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rmpc_mean'], 4), label='MPC: {:.2f}'.format(sum(r['rmpc_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rmpc_mean'])), r['rmpc_mean']+r['rmpc_std'], r['rmpc_mean']-r['rmpc_std'], color=p[0].get_color(), alpha=0.3)\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Time steps')\n",
    "plt.legend()\n",
    "plt.title('Transfer: tower 1 to 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Transfer Inside Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Chiller/tower data to use\n",
    "ch = ch2\n",
    "ticker = ticker2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "similarity_idx = ['TempWetBulb', 'TempAmbient', 'Tonnage'] # variables for judging env similarity in ticker\n",
    "similarity_matrix = stats.timeseries.similarity_matrix(ticker, similarity_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clusterer = cluster.SpectralClustering(n_clusters=2, affinity='precomputed')\n",
    "cluster_labels = clusterer.fit_predict(similarity_matrix)\n",
    "projecter = manifold.SpectralEmbedding(affinity='precomputed')\n",
    "coords = projecter.fit_transform(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "coords0 = coords[cluster_labels==0]\n",
    "coords1 = coords[cluster_labels==1]\n",
    "plt.scatter(coords0[:,0], coords0[:,1], label='Cluster A')\n",
    "plt.scatter(coords1[:,0], coords1[:,1], label='Cluster B')\n",
    "plt.legend()\n",
    "plt.title('Episodes clustered by\\n independent state variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Env A Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tickera = [t for label, t in zip(cluster_labels, ticker) if label==0]\n",
    "cha = ch.loc[pd.concat(tickera).index]\n",
    "x_traina, x_vala, y_traina, y_vala, _, scalera = get_env_data(cha, colsx, colsy, ticker_vars)\n",
    "modela = train_model(x_traina, y_traina)\n",
    "enva = get_env(modela, scalera, tickera, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training on environment a\n",
    "agenta = rl.PPO(env=enva, **agent_params)\n",
    "rewardsa = agenta.learn(timesteps=timesteps)\n",
    "plt.plot(rewardsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Env B Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tickerb = [t for label, t in zip(cluster_labels, ticker) if label==1]\n",
    "chb = ch.loc[pd.concat(tickerb).index]\n",
    "x_trainb, x_valb, y_trainb, y_valb, _, scalerb = get_env_data(chb, colsx, colsy, ticker_vars)\n",
    "modelb = train_model(x_trainb, y_trainb)\n",
    "envb = get_env(modelb, scalerb, tickerb, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training on environment b\n",
    "agentb = rl.PPO(env=envb, **agent_params)\n",
    "rewardsb = agentb.learn(timesteps=timesteps)\n",
    "plt.plot(rewardsb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Model Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model 1 prediction with model 2 data\n",
    "vary = (0, 5)\n",
    "sa = model_surface(lambda x: modela.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "sb = model_surface(lambda x: modelb.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "\n",
    "# fitting model1 on model2 data\n",
    "modelab = helpers.clone(modela, dict(n_iter_=0, t_=0, loss_curve_=[], best_loss_=np.inf))\n",
    "modelab.set_params(warm_start=True, verbose=True)\n",
    "modelab.fit(x_valb, y_valb)\n",
    "# modelab.fit(x_trainb, y_trainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "sab = model_surface(lambda x: modelab.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "\n",
    "plot_surface(*sa, fig_kwargs={'figsize':(8,8)}, cmap=plt.cm.coolwarm, hatch='o', alpha=0.6)\n",
    "ax = plt.gca()\n",
    "plot_surface(*sb, cmap=plt.cm.coolwarm, ax=ax, hatch='-', alpha=0.6)\n",
    "plot_surface(*sab, cmap=plt.cm.coolwarm, ax=ax, hatch='.', alpha=0.6)\n",
    "\n",
    "ax.set_zlim(50, 80)\n",
    "plt.xlabel(colsx[vary[0]])\n",
    "plt.ylabel(colsx[vary[1]] if vary[1]<len(colsx) else 'Setpoint')\n",
    "ax.set_zlabel('Condenser Temperature In')\n",
    "ax.view_init(10, 60)\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Policy transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transferring across\n",
    "(agentab, rewardsab), (agentab_, rewardsab_) = \\\n",
    "    transfer_experiment(agenta, agentb, transfer_model=modelab, timesteps=288*30, **agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Learning performance\n",
    "plt.plot(stats.rolling_mean(rewardsa, 4), label='Tower A: %.2f' % np.mean(rewardsa[-5:]), ls=':')\n",
    "plt.plot(stats.rolling_mean(rewardsb, 4), label='Tower B: %.2f' % np.mean(rewardsb[-5:]))\n",
    "plt.plot(stats.rolling_mean(rewardsab, 4), label='Tower 1->model->2: %.2f' % np.mean(rewardsab[-5:]))\n",
    "plt.plot(stats.rolling_mean(rewardsab_, 4), label='Tower A->B: %.2f' % np.mean(rewardsab_[-5:]))\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.title('RL training performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Operational performance\n",
    "results = dict(ra=[], rb=[], rab=[], rab_=[], rud=[], rfa=[], rmpc=[])\n",
    "for i in trange(10, leave=False):\n",
    "    _seed = i\n",
    "    results['ra'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: agenta.predict(x)[0])['reward'][0])\n",
    "    results['rb'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: agentb.predict(x)[0])['reward'][0])\n",
    "    results['rab'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: agentab.predict(x)[0])['reward'][0])\n",
    "    results['rab_'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: agentab_.predict(x)[0])['reward'][0])\n",
    "\n",
    "    updown = UpDownController(modelb.predict, scalerb.transform, 0)\n",
    "    results['rud'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: updown.predict(x)[0])['reward'][0])\n",
    "    fixed = FixedApproachController(approach=5)\n",
    "    results['rfa'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: fixed.predict(x)[0])['reward'][0])\n",
    "    mpc = ModelPredictiveController(modelb.predict, scalerb.transform, resolution=21)\n",
    "    results['rmpc'].append(helpers.get_from_env(('reward',), get_env(modelb, scalerb, tickerb, _seed), lambda x: mpc.predict(x)[0])['reward'][0])\n",
    "\n",
    "keys = list(results.keys())\n",
    "resaggab = {}\n",
    "for key in keys:\n",
    "    results[key] = helpers.homogenous_array(results[key])\n",
    "    resaggab[key+'_mean'] = np.nanmean(results[key], axis=0)\n",
    "    resaggab[key+'_std'] = np.nanstd(results[key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "r = resaggab\n",
    "p = plt.plot(stats.rolling_mean(r['rb_mean'], 4), label='Tower B: {:.2f}'.format(sum(r['rb_mean'])), ls=':')\n",
    "plt.fill_between(np.arange(len(r['rb_mean'])), r['rb_mean']+r['rb_std'], r['rb_mean']-r['rb_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rab_mean'], 4), label='Tower A->model->B: {:.2f}'.format(sum(r['rab_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rab_mean'])), r['rab_mean']+r['rab_std'], r['rab_mean']-r['rab_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rab__mean'], 4), label='Tower A->B: {:.2f}'.format(sum(r['rab__mean'])))\n",
    "plt.fill_between(np.arange(len(r['rab__mean'])), r['rab__mean']+r['rab__std'], r['rab__mean']-r['rab__std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rud_mean'], 4), label='UpDown: {:.2f}'.format(sum(r['rud_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rud_mean'])), r['rud_mean']+r['rud_std'], r['rud_mean']-r['rud_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rfa_mean'], 4), label='Fixed: {:.2f}'.format(sum(r['rfa_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rfa_mean'])), r['rfa_mean']+r['rfa_std'], r['rfa_mean']-r['rfa_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rmpc_mean'], 4), label='MPC: {:.2f}'.format(sum(r['rmpc_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rmpc_mean'])), r['rmpc_mean']+r['rmpc_std'], r['rmpc_mean']-r['rmpc_std'], color=p[0].get_color(), alpha=0.3)\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Time steps')\n",
    "plt.legend()\n",
    "plt.title('Transfer: Cluster A to B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Transfer using stale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# `old` or `o` suffix for old,\n",
    "# `new` or `n` suffix for new\n",
    "chold, chnew = ch1_ref, ch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "bins = np.linspace(55, 75, num=11, endpoint=True)\n",
    "plt.hist(chold['Setpoint'], label='Old control', density=True, bins=bins, color='r', alpha=0.5)\n",
    "plt.hist(chnew['Setpoint'], label='Up-down control (new)', density=True, bins=bins, color='b', alpha=0.5)\n",
    "plt.xlabel('Setpoint /F')\n",
    "plt.ylabel('Relative frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Policy on old env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_traino, x_valo, y_traino, y_valo, tickero, scalero = get_env_data(chold, colsx, colsy, ticker_vars)\n",
    "modelo = train_model(x_traino, y_traino)\n",
    "envo = get_env(modelo, scalero, tickero, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Training on environment old\n",
    "agento = rl.PPO(env=envo, **agent_params)\n",
    "rewardso = agento.learn(timesteps=timesteps)\n",
    "plt.plot(rewardso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Policy on new env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_trainn, x_valn, y_trainn, y_valn, tickern, scalern = get_env_data(chnew, colsx, colsy, ticker_vars)\n",
    "modeln = train_model(x_trainn, y_trainn)\n",
    "envn = get_env(modeln, scalern, tickern, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Training on environment new\n",
    "agentn = rl.PPO(env=envn, **agent_params)\n",
    "rewardsn = agentn.learn(timesteps=timesteps)\n",
    "plt.plot(rewardsn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Model Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model 1 prediction with model 2 data\n",
    "vary = (0, 5)\n",
    "so = model_surface(lambda x: modelo.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "sn = model_surface(lambda x: modeln.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "\n",
    "# fitting modelo on modeln data\n",
    "modelon = helpers.clone(modelo, dict(n_iter_=0, t_=0, loss_curve_=[], best_loss_=np.inf))\n",
    "modelon.set_params(warm_start=True, verbose=True)\n",
    "modelon.fit(x_valn, y_valn)\n",
    "# modelon.fit(x_trainn, y_trainn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "son = model_surface(lambda x: modelon.predict(x)[:,0], test_point, vary, ((-1,1),(-1,1)), (10,10))\n",
    "\n",
    "plot_surface(*so, fig_kwargs={'figsize':(8,8)}, cmap=plt.cm.coolwarm, hatch='o', alpha=0.6)\n",
    "ax = plt.gca()\n",
    "plot_surface(*sn, cmap=plt.cm.coolwarm, ax=ax, hatch='-', alpha=0.6)\n",
    "plot_surface(*son, cmap=plt.cm.coolwarm, ax=ax, hatch='.', alpha=0.6)\n",
    "\n",
    "plt.xlabel(colsx[vary[0]])\n",
    "plt.ylabel(colsx[vary[1]] if vary[1]<len(colsx) else 'Setpoint')\n",
    "ax.set_zlim(50, 80)\n",
    "ax.set_zlabel('Condenser Temperature In')\n",
    "ax.view_init(10, 60)\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Policy transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transferring across\n",
    "(agenton, rewardson), (agenton_, rewardson_) = \\\n",
    "    transfer_experiment(agento, agentn, transfer_model=None, timesteps=timesteps, **agent_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Learning performance\n",
    "plt.plot(stats.rolling_mean(rewardso, 4), label='Old data: %.2f' % np.mean(rewardso[-5:]), ls=':')\n",
    "plt.plot(stats.rolling_mean(rewardsn, 4), label='New data: %.2f' % np.mean(rewardsn[-5:]))\n",
    "# plt.plot(stats.rolling_mean(rewardson, 4), label='Old data->model->New data: %.2f' % np.mean(rewardson[-5:]))\n",
    "plt.plot(stats.rolling_mean(rewardson_, 4), label='Old data->New data: %.2f' % np.mean(rewardson_[-5:]))\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.title('RL Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Operational performance\n",
    "results = dict(ro=[], rn=[], ron=[], ron_=[], rud=[], rfa=[], rmpc=[])\n",
    "for i in trange(10, leave=False):\n",
    "    _seed = i\n",
    "    results['ro'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: agento.predict(x)[0])['reward'][0])\n",
    "    results['rn'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: agentn.predict(x)[0])['reward'][0])\n",
    "#     results['ron'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: agenton.predict(x)[0])['reward'][0])\n",
    "    results['ron_'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: agenton_.predict(x)[0])['reward'][0])\n",
    "\n",
    "    updown = UpDownController(modeln.predict, scalern.transform, 0)\n",
    "    results['rud'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: updown.predict(x)[0])['reward'][0])\n",
    "    fixed = FixedApproachController(approach=5)\n",
    "    results['rfa'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: fixed.predict(x)[0])['reward'][0])\n",
    "    mpc = ModelPredictiveController(modelb.predict, scalerb.transform, resolution=21)\n",
    "    results['rmpc'].append(helpers.get_from_env(('reward',), get_env(modeln, scalern, tickern, _seed), lambda x: mpc.predict(x)[0])['reward'][0])\n",
    "\n",
    "keys = list(results.keys())\n",
    "resaggon = {}\n",
    "for key in keys:\n",
    "    if len(results[key])==0: continue\n",
    "    results[key] = helpers.homogenous_array(results[key])\n",
    "    resaggon[key+'_mean'] = np.nanmean(results[key], axis=0)\n",
    "    resaggon[key+'_std'] = np.nanstd(results[key], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "r = resaggon\n",
    "p = plt.plot(stats.rolling_mean(r['rn_mean'], 4), label='Diverse: {:.2f}'.format(sum(r['rn_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rn_mean'])), r['rn_mean']+r['rn_std'], r['rn_mean']-r['rn_std'], color=p[0].get_color(), alpha=0.3)\n",
    "# p = plt.plot(stats.rolling_mean(r['ron_mean'], 4), label='Tower A->model->B: {:.2f}'.format(sum(r['ron_mean'])))\n",
    "# plt.fill_between(np.arange(len(r['ron_mean'])), r['ron_mean']+r['ron_std'], r['ron_mean']-r['ron_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['ron__mean'], 4), label='Sparse->Diverse: {:.2f}'.format(sum(r['ron__mean'])))\n",
    "plt.fill_between(np.arange(len(r['ron__mean'])), r['ron__mean']+r['ron__std'], r['ron__mean']-r['ron__std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rud_mean'], 4), label='UpDown: {:.2f}'.format(sum(r['rud_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rud_mean'])), r['rud_mean']+r['rud_std'], r['rud_mean']-r['rud_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rfa_mean'], 4), label='Fixed: {:.2f}'.format(sum(r['rfa_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rfa_mean'])), r['rfa_mean']+r['rfa_std'], r['rfa_mean']-r['rfa_std'], color=p[0].get_color(), alpha=0.3)\n",
    "p = plt.plot(stats.rolling_mean(r['rmpc_mean'], 4), label='MPC: {:.2f}'.format(sum(r['rmpc_mean'])))\n",
    "plt.fill_between(np.arange(len(r['rmpc_mean'])), r['rmpc_mean']+r['rmpc_std'], r['rmpc_mean']-r['rmpc_std'], color=p[0].get_color(), alpha=0.3)\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Time steps')\n",
    "plt.legend()\n",
    "plt.title('Transfer: sparse to diverse data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
